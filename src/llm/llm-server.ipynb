{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGXF5Stf-p92"
      },
      "outputs": [],
      "source": [
        "# fastAPIのインストール\n",
        "!pip uninstall tensorflow-probability -y\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn-GGp60AC0D"
      },
      "outputs": [],
      "source": [
        "#LLMのインストール\n",
        "! pip install transformers sentencepiece accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctvcfRRFAM4E"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class OnAction(BaseModel):\n",
        "    client_uuid: str\n",
        "    search_word: str\n",
        "    page_content: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWQn9gx0AOsP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "class Agent:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"rinna/bilingual-gpt-neox-4b-instruction-ppo\",\n",
        "        use_fast=False\n",
        "    )\n",
        "    def __init__(self):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"rinna/bilingual-gpt-neox-4b-instruction-ppo\",\n",
        "            load_in_8bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "        self.query_history = []\n",
        "        self.answer_history = []\n",
        "\n",
        "    def on_user_action(self, user_action: OnAction):\n",
        "        try :\n",
        "            # プロンプトの準備\n",
        "            prompt = \"\"\"\n",
        "これから、あるプログラマーが開発の中で行き詰まった時に検索した言葉を「検索ワード:」に続いて、また、その検索ワードの下で訪れたwebページの冒頭を「訪問ページ冒頭:」の形式で与えます。これらを順次与えますが、直前までと同じ課題について検索している時は”True”を、検索して解決しようとしている課題が切り替わった時には”False”を出力してください。以下に、例をいくつか示します。私が与える文章の先頭には”Q:”を、あなたが出力する文章の先頭には”A:”をつけています。この入力に対してあなたは”True”または”False”以外を出力していはいけません。\n",
        "Q:検索ワード:chrome.tabs.onUpdated.addListener\n",
        "A:True\n",
        "\n",
        "Q:検索ワード:mozilla tabs.onUpdated\n",
        "A:False\n",
        "\n",
        "Q:検索ワード:chrome拡張 作り方\n",
        "A:True\n",
        "です。では、始めます。\n",
        "\"\"\"\n",
        "            for i in range(len(self.query_history)):\n",
        "                prompt += \"Q:検索ワード:\" + self.query_history[i] + \"\\nA:\" + str(self.answer_history[i]) + \"\\n\\n\"\n",
        "            prompt += \"Q:検索ワード:\" + user_action.search_word + \"A:\"\n",
        "            # 推論の実行\n",
        "            token_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                output_ids = self.model.generate(\n",
        "                    token_ids.to(self.model.device),\n",
        "                    max_new_tokens=512,\n",
        "                    do_sample=True,\n",
        "                    temperature=1.0,\n",
        "                    top_p=0.85,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    bos_token_id=self.tokenizer.bos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "            output = self.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
        "            print(\"output:\"+output)\n",
        "\n",
        "            is_same_topic = True if output == \"True</s>\" else False\n",
        "            self.query_history.append(user_action.search_word)\n",
        "            self.answer_history.append(is_same_topic)\n",
        "            return is_same_topic\n",
        "        except :\n",
        "            print(\"LLM error!\")\n",
        "            self.query_history.append(user_action.search_word)\n",
        "            self.answer_history.append(is_same_topic)\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9IawmdE_izz"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uuid, random\n",
        "# from LLM import Agent\n",
        "\n",
        "# fastapiを実行\n",
        "app = FastAPI()\n",
        "\n",
        "# CORSミドルウェアを有効にする\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Agent情報を辞書形式で保持\n",
        "agent_dict = {}\n",
        "\n",
        "# BaseModelをインポート\n",
        "#from models import OnAction\n",
        "\n",
        "# 自然言語モデルに対する処理\n",
        "@app.post(\"/on_action\")\n",
        "async def on_action(req: OnAction):\n",
        "    client_uuid_str = str(req.client_uuid)\n",
        "    # 空文字列を受け取るとuuidを新規発行\n",
        "    if client_uuid_str == \"\":\n",
        "        client_uuid_str = str(uuid.uuid4())\n",
        "        agent_dict[client_uuid_str] = Agent()\n",
        "\n",
        "\n",
        "    is_same_topic = agent_dict[client_uuid_str].on_user_action(req)\n",
        "    response_data = {\"is_same_topic\": is_same_topic, \"client_uuid\": client_uuid_str}\n",
        "\n",
        "    return JSONResponse(content=response_data, status_code=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDaiSJzUDX_a"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
